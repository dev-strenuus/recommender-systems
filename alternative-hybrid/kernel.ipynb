{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "50f8ae9a001b0f4caa45837c796199275e42e984"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on 23/10/17\n",
    "\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import time, sys\n",
    "import scipy.sparse as sps\n",
    "\n",
    "\n",
    "\n",
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)\n",
    "\n",
    "\n",
    "\n",
    "class Compute_Similarity_Python:\n",
    "\n",
    "\n",
    "    def __init__(self, dataMatrix, topK=100, shrink = 0, normalize = True,\n",
    "                 asymmetric_alpha = 0.5, tversky_alpha = 1.0, tversky_beta = 1.0,\n",
    "                 similarity = \"cosine\", row_weights = None):\n",
    "        \"\"\"\n",
    "        Computes the cosine similarity on the columns of dataMatrix\n",
    "        If it is computed on URM=|users|x|items|, pass the URM as is.\n",
    "        If it is computed on ICM=|items|x|features|, pass the ICM transposed.\n",
    "        :param dataMatrix:\n",
    "        :param topK:\n",
    "        :param shrink:\n",
    "        :param normalize:           If True divide the dot product by the product of the norms\n",
    "        :param row_weights:         Multiply the values in each row by a specified value. Array\n",
    "        :param asymmetric_alpha     Coefficient alpha for the asymmetric cosine\n",
    "        :param similarity:  \"cosine\"        computes Cosine similarity\n",
    "                            \"adjusted\"      computes Adjusted Cosine, removing the average of the users\n",
    "                            \"asymmetric\"    computes Asymmetric Cosine\n",
    "                            \"pearson\"       computes Pearson Correlation, removing the average of the items\n",
    "                            \"jaccard\"       computes Jaccard similarity for binary interactions using Tanimoto\n",
    "                            \"dice\"          computes Dice similarity for binary interactions\n",
    "                            \"tversky\"       computes Tversky similarity for binary interactions\n",
    "                            \"tanimoto\"      computes Tanimoto coefficient for binary interactions\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Asymmetric Cosine as described in: \n",
    "        Aiolli, F. (2013, October). Efficient top-n recommendation for very large scale binary rated datasets. In Proceedings of the 7th ACM conference on Recommender systems (pp. 273-280). ACM.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        super(Compute_Similarity_Python, self).__init__()\n",
    "\n",
    "        self.TopK = topK\n",
    "        self.shrink = shrink\n",
    "        self.normalize = normalize\n",
    "        self.n_columns = dataMatrix.shape[1]\n",
    "        self.n_rows = dataMatrix.shape[0]\n",
    "        self.asymmetric_alpha = asymmetric_alpha\n",
    "        self.tversky_alpha = tversky_alpha\n",
    "        self.tversky_beta = tversky_beta\n",
    "\n",
    "        self.dataMatrix = dataMatrix.copy()\n",
    "\n",
    "        self.adjusted_cosine = False\n",
    "        self.asymmetric_cosine = False\n",
    "        self.pearson_correlation = False\n",
    "        self.tanimoto_coefficient = False\n",
    "        self.dice_coefficient = False\n",
    "        self.tversky_coefficient = False\n",
    "\n",
    "        if similarity == \"adjusted\":\n",
    "            self.adjusted_cosine = True\n",
    "        elif similarity == \"asymmetric\":\n",
    "            self.asymmetric_cosine = True\n",
    "        elif similarity == \"pearson\":\n",
    "            self.pearson_correlation = True\n",
    "        elif similarity == \"jaccard\" or similarity == \"tanimoto\":\n",
    "            self.tanimoto_coefficient = True\n",
    "            # Tanimoto has a specific kind of normalization\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"dice\":\n",
    "            self.dice_coefficient = True\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"tversky\":\n",
    "            self.tversky_coefficient = True\n",
    "            self.normalize = False\n",
    "\n",
    "        elif similarity == \"cosine\":\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Cosine_Similarity: value for paramether 'mode' not recognized.\"\n",
    "                             \" Allowed values are: 'cosine', 'pearson', 'adjusted', 'asymmetric', 'jaccard', 'tanimoto',\"\n",
    "                             \"dice, tversky.\"\n",
    "                             \" Passed value was '{}'\".format(similarity))\n",
    "\n",
    "\n",
    "\n",
    "        if self.TopK == 0:\n",
    "            self.W_dense = np.zeros((self.n_columns, self.n_columns))\n",
    "\n",
    "\n",
    "        self.use_row_weights = False\n",
    "\n",
    "        if row_weights is not None:\n",
    "\n",
    "            if dataMatrix.shape[0] != len(row_weights):\n",
    "                raise ValueError(\"Cosine_Similarity: provided row_weights and dataMatrix have different number of rows.\"\n",
    "                                 \"Col_weights has {} columns, dataMatrix has {}.\".format(len(row_weights), dataMatrix.shape[0]))\n",
    "\n",
    "            self.use_row_weights = True\n",
    "            self.row_weights = row_weights.copy()\n",
    "            self.row_weights_diag = sps.diags(self.row_weights)\n",
    "\n",
    "            self.dataMatrix_weighted = self.dataMatrix.T.dot(self.row_weights_diag).T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def applyAdjustedCosine(self):\n",
    "        \"\"\"\n",
    "        Remove from every data point the average for the corresponding row\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csr')\n",
    "\n",
    "\n",
    "        interactionsPerRow = np.diff(self.dataMatrix.indptr)\n",
    "\n",
    "        nonzeroRows = interactionsPerRow > 0\n",
    "        sumPerRow = np.asarray(self.dataMatrix.sum(axis=1)).ravel()\n",
    "\n",
    "        rowAverage = np.zeros_like(sumPerRow)\n",
    "        rowAverage[nonzeroRows] = sumPerRow[nonzeroRows] / interactionsPerRow[nonzeroRows]\n",
    "\n",
    "\n",
    "        # Split in blocks to avoid duplicating the whole data structure\n",
    "        start_row = 0\n",
    "        end_row= 0\n",
    "\n",
    "        blockSize = 1000\n",
    "\n",
    "\n",
    "        while end_row < self.n_rows:\n",
    "\n",
    "            end_row = min(self.n_rows, end_row + blockSize)\n",
    "\n",
    "            self.dataMatrix.data[self.dataMatrix.indptr[start_row]:self.dataMatrix.indptr[end_row]] -= \\\n",
    "                np.repeat(rowAverage[start_row:end_row], interactionsPerRow[start_row:end_row])\n",
    "\n",
    "            start_row += blockSize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def applyPearsonCorrelation(self):\n",
    "        \"\"\"\n",
    "        Remove from every data point the average for the corresponding column\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n",
    "\n",
    "\n",
    "        interactionsPerCol = np.diff(self.dataMatrix.indptr)\n",
    "\n",
    "        nonzeroCols = interactionsPerCol > 0\n",
    "        sumPerCol = np.asarray(self.dataMatrix.sum(axis=0)).ravel()\n",
    "\n",
    "        colAverage = np.zeros_like(sumPerCol)\n",
    "        colAverage[nonzeroCols] = sumPerCol[nonzeroCols] / interactionsPerCol[nonzeroCols]\n",
    "\n",
    "\n",
    "        # Split in blocks to avoid duplicating the whole data structure\n",
    "        start_col = 0\n",
    "        end_col= 0\n",
    "\n",
    "        blockSize = 1000\n",
    "\n",
    "\n",
    "        while end_col < self.n_columns:\n",
    "\n",
    "            end_col = min(self.n_columns, end_col + blockSize)\n",
    "\n",
    "            self.dataMatrix.data[self.dataMatrix.indptr[start_col]:self.dataMatrix.indptr[end_col]] -= \\\n",
    "                np.repeat(colAverage[start_col:end_col], interactionsPerCol[start_col:end_col])\n",
    "\n",
    "            start_col += blockSize\n",
    "\n",
    "\n",
    "    def useOnlyBooleanInteractions(self):\n",
    "\n",
    "        # Split in blocks to avoid duplicating the whole data structure\n",
    "        start_pos = 0\n",
    "        end_pos= 0\n",
    "\n",
    "        blockSize = 1000\n",
    "\n",
    "\n",
    "        while end_pos < len(self.dataMatrix.data):\n",
    "\n",
    "            end_pos = min(len(self.dataMatrix.data), end_pos + blockSize)\n",
    "\n",
    "            self.dataMatrix.data[start_pos:end_pos] = np.ones(end_pos-start_pos)\n",
    "\n",
    "            start_pos += blockSize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_similarity(self, start_col=None, end_col=None, block_size = 100):\n",
    "        \"\"\"\n",
    "        Compute the similarity for the given dataset\n",
    "        :param self:\n",
    "        :param start_col: column to begin with\n",
    "        :param end_col: column to stop before, end_col is excluded\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        values = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_time_print_batch = start_time\n",
    "        processedItems = 0\n",
    "\n",
    "\n",
    "        if self.adjusted_cosine:\n",
    "            self.applyAdjustedCosine()\n",
    "\n",
    "        elif self.pearson_correlation:\n",
    "            self.applyPearsonCorrelation()\n",
    "\n",
    "        elif self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient:\n",
    "            self.useOnlyBooleanInteractions()\n",
    "\n",
    "\n",
    "        # We explore the matrix column-wise\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n",
    "\n",
    "\n",
    "        # Compute sum of squared values to be used in normalization\n",
    "        sumOfSquared = np.array(self.dataMatrix.power(2).sum(axis=0)).ravel()\n",
    "\n",
    "        # Tanimoto does not require the square root to be applied\n",
    "        if not (self.tanimoto_coefficient or self.dice_coefficient or self.tversky_coefficient):\n",
    "            sumOfSquared = np.sqrt(sumOfSquared)\n",
    "\n",
    "        if self.asymmetric_cosine:\n",
    "            sumOfSquared_to_1_minus_alpha = sumOfSquared.power(2 * (1 - self.asymmetric_alpha))\n",
    "            sumOfSquared_to_alpha = sumOfSquared.power(2 * self.asymmetric_alpha)\n",
    "\n",
    "\n",
    "        self.dataMatrix = check_matrix(self.dataMatrix, 'csc')\n",
    "\n",
    "        start_col_local = 0\n",
    "        end_col_local = self.n_columns\n",
    "\n",
    "        if start_col is not None and start_col>0 and start_col<self.n_columns:\n",
    "            start_col_local = start_col\n",
    "\n",
    "        if end_col is not None and end_col>start_col_local and end_col<self.n_columns:\n",
    "            end_col_local = end_col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        start_col_block = start_col_local\n",
    "\n",
    "        this_block_size = 0\n",
    "\n",
    "        # Compute all similarities for each item using vectorization\n",
    "        while start_col_block < end_col_local:\n",
    "\n",
    "            # Add previous block size\n",
    "            processedItems += this_block_size\n",
    "\n",
    "            end_col_block = min(start_col_block + block_size, end_col_local)\n",
    "            this_block_size = end_col_block-start_col_block\n",
    "\n",
    "\n",
    "            if time.time() - start_time_print_batch >= 30 or end_col_block==end_col_local:\n",
    "                columnPerSec = processedItems / (time.time() - start_time)\n",
    "\n",
    "                print(\"Similarity column {} ( {:2.0f} % ), {:.2f} column/sec, elapsed time {:.2f} min\".format(\n",
    "                    processedItems, processedItems / (end_col_local - start_col_local) * 100, columnPerSec, (time.time() - start_time)/ 60))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time_print_batch = time.time()\n",
    "\n",
    "\n",
    "            # All data points for a given item\n",
    "            item_data = self.dataMatrix[:, start_col_block:end_col_block]\n",
    "            item_data = item_data.toarray().squeeze()\n",
    "\n",
    "            if self.use_row_weights:\n",
    "                #item_data = np.multiply(item_data, self.row_weights)\n",
    "                #item_data = item_data.T.dot(self.row_weights_diag).T\n",
    "                this_block_weights = self.dataMatrix_weighted.T.dot(item_data)\n",
    "\n",
    "            else:\n",
    "                # Compute item similarities\n",
    "                this_block_weights = self.dataMatrix.T.dot(item_data)\n",
    "\n",
    "\n",
    "\n",
    "            for col_index_in_block in range(this_block_size):\n",
    "\n",
    "                if this_block_size == 1:\n",
    "                    this_column_weights = this_block_weights\n",
    "                else:\n",
    "                    this_column_weights = this_block_weights[:,col_index_in_block]\n",
    "\n",
    "\n",
    "                columnIndex = col_index_in_block + start_col_block\n",
    "                this_column_weights[columnIndex] = 0.0\n",
    "\n",
    "                # Apply normalization and shrinkage, ensure denominator != 0\n",
    "                if self.normalize:\n",
    "\n",
    "                    if self.asymmetric_cosine:\n",
    "                        denominator = sumOfSquared_to_alpha[columnIndex] * sumOfSquared_to_1_minus_alpha + self.shrink + 1e-6\n",
    "                    else:\n",
    "                        denominator = sumOfSquared[columnIndex] * sumOfSquared + self.shrink + 1e-6\n",
    "\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "\n",
    "                # Apply the specific denominator for Tanimoto\n",
    "                elif self.tanimoto_coefficient:\n",
    "                    denominator = sumOfSquared[columnIndex] + sumOfSquared - this_column_weights + self.shrink + 1e-6\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "                elif self.dice_coefficient:\n",
    "                    denominator = sumOfSquared[columnIndex] + sumOfSquared + self.shrink + 1e-6\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "                elif self.tversky_coefficient:\n",
    "                    denominator = this_column_weights + \\\n",
    "                                  (sumOfSquared[columnIndex] - this_column_weights)*self.tversky_alpha + \\\n",
    "                                  (sumOfSquared - this_column_weights)*self.tversky_beta + self.shrink + 1e-6\n",
    "                    this_column_weights = np.multiply(this_column_weights, 1 / denominator)\n",
    "\n",
    "                # If no normalization or tanimoto is selected, apply only shrink\n",
    "                elif self.shrink != 0:\n",
    "                    this_column_weights = this_column_weights/self.shrink\n",
    "\n",
    "\n",
    "                #this_column_weights = this_column_weights.toarray().ravel()\n",
    "\n",
    "                if self.TopK == 0:\n",
    "                    self.W_dense[:, columnIndex] = this_column_weights\n",
    "\n",
    "                else:\n",
    "                    # Sort indices and select TopK\n",
    "                    # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "                    # - Partition the data to extract the set of relevant items\n",
    "                    # - Sort only the relevant items\n",
    "                    # - Get the original item index\n",
    "                    relevant_items_partition = (-this_column_weights).argpartition(self.TopK-1)[0:self.TopK]\n",
    "                    relevant_items_partition_sorting = np.argsort(-this_column_weights[relevant_items_partition])\n",
    "                    top_k_idx = relevant_items_partition[relevant_items_partition_sorting]\n",
    "\n",
    "                    # Incrementally build sparse matrix, do not add zeros\n",
    "                    notZerosMask = this_column_weights[top_k_idx] != 0.0\n",
    "                    numNotZeros = np.sum(notZerosMask)\n",
    "\n",
    "                    values.extend(this_column_weights[top_k_idx][notZerosMask])\n",
    "                    rows.extend(top_k_idx[notZerosMask])\n",
    "                    cols.extend(np.ones(numNotZeros) * columnIndex)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            start_col_block += block_size\n",
    "\n",
    "        # End while on columns\n",
    "\n",
    "\n",
    "        if self.TopK == 0:\n",
    "            return self.W_dense\n",
    "\n",
    "        else:\n",
    "\n",
    "            W_sparse = sps.csr_matrix((values, (rows, cols)),\n",
    "                                      shape=(self.n_columns, self.n_columns),\n",
    "                                      dtype=np.float32)\n",
    "\n",
    "\n",
    "            return W_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7ae15d2d7d73494bef77de6ea637ae10786aa76"
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "import numpy as np\n",
    "class ItemCBFKNNRecommender(object):\n",
    "    \n",
    "    def __init__(self, URM, ICM):\n",
    "        self.URM = URM\n",
    "        self.ICM = ICM\n",
    "    \n",
    "    \n",
    "    def fit(self, topK=50, shrink=100, normalize = True, similarity = \"cosine\"):\n",
    "        \n",
    "        similarity_object = Compute_Similarity_Python(self.ICM.T, shrink=shrink,\n",
    "        topK=topK, normalize=normalize,\n",
    "        similarity = similarity)\n",
    "\n",
    "        self.W_sparse = similarity_object.compute_similarity()\n",
    "        \n",
    "        #print (self.W_sparse)\n",
    "        #sparse.save_npz('cbsim.npz', self.W_sparse, compressed=True)\n",
    "        #sparse_matrix = sparse.load_npz('cbsim.npz')\n",
    "        #print (sparse_matrix)\n",
    "        return self.W_sparse\n",
    "    \n",
    "    \n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM[user_id]\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "        \n",
    "        if exclude_seen:\n",
    "            scores = self.filter_seen(user_id, scores)\n",
    "        \n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "        \n",
    "        return ranking[:at]\n",
    "    \n",
    "    \n",
    "    def filter_seen(self, user_id, scores):\n",
    "        \n",
    "        start_pos = self.URM.indptr[user_id]\n",
    "        end_pos = self.URM.indptr[user_id+1]\n",
    "        \n",
    "        user_profile = self.URM.indices[start_pos:end_pos]\n",
    "        \n",
    "        scores[user_profile] = -np.inf\n",
    "        \n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "adf251892931b5b8e3a6aa68c701f24a456fa46e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse as sps\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, normalize\n",
    "from sklearn import feature_extraction\n",
    "\n",
    "\n",
    "class Builder(object):\n",
    "    \n",
    "    def __init__(self, local):\n",
    "        if local == True:\n",
    "            self.train= pd.read_csv('../input/input-csv/train_holdout.csv')\n",
    "            self.test= pd.read_csv('../input/input-csv/test_holdout.csv')\n",
    "        else:\n",
    "            self.train = pd.read_csv('../input/input-csv/new_train.csv')\n",
    "        self.target_playlists = pd.read_csv('../input/input-csv/target_playlists.csv')\n",
    "        self.tracks = pd.read_csv('../input/input-csv/tracks.csv')\n",
    "        #self.ordered_train=pd.read_csv('input/train_sequential.csv')\n",
    "        #for playlist in np.array(self.target_playlists['playlist_id'])[0:5000]:\n",
    "        #    self.train = self.train[self.train.playlist_id!=playlist]\n",
    "        #self.train = self.train.append(self.ordered_train)\n",
    "        #print (self.train[self.train['playlist_id']==7]['track_id'])\n",
    "        self.playlists = self.get_playlists()\n",
    "        #self.tracks_inside_playlists_train = np.empty((len(self.playlists)), dtype=object)\n",
    "\n",
    "    def get_train_pd(self):\n",
    "        return self.train\n",
    "\n",
    "    def get_target_playlists_pd(self):\n",
    "        return self.target_playlists\n",
    "\n",
    "    def get_tracks_pd(self):\n",
    "        return self.tracks\n",
    "\n",
    "    def get_ordered_target_playlists(self):\n",
    "        return np.array(self.target_playlists['playlist_id'])[0:5000]\n",
    "\n",
    "    def get_unordered_target_playlists(self):\n",
    "        return np.array(self.target_playlists['playlist_id'])[5000:]\n",
    "\n",
    "    def get_tracks_inside_playlist_train(self, playlist):\n",
    "        return np.array(self.train[self.train['playlist_id']==playlist]['track_id'])\n",
    "    \n",
    "    def get_tracks(self):\n",
    "        tracks = self.tracks['track_id'].unique()\n",
    "        return np.sort(tracks)\n",
    "    \n",
    "    def get_playlists(self):\n",
    "        playlists = self.train['playlist_id'].unique()\n",
    "        return np.sort(playlists)\n",
    "    \n",
    "    def get_target_playlists(self):\n",
    "        target_playlists = self.target_playlists['playlist_id'].unique()\n",
    "        return np.sort(target_playlists)\n",
    "    \n",
    "    def get_artists(self):\n",
    "        artists = self.tracks['artist_id'].unique()\n",
    "        return np.sort(artists)\n",
    "    \n",
    "    def get_albums(self):\n",
    "        albums = self.tracks['album_id'].unique()\n",
    "        return np.sort(albums)\n",
    "    \n",
    "    def get_durations(self):\n",
    "        durations = self.tracks['duration_sec'].unique()\n",
    "        return np.sort(durations)\n",
    "    \n",
    "    def get_URM_test(self):\n",
    "        playlistsSize = len(self.get_playlists())\n",
    "        tracksSize = len(self.get_tracks())\n",
    "        URM_test_row = np.zeros(self.test.shape[0])\n",
    "        URM_test_col = np.zeros(self.test.shape[0])\n",
    "        URM_test_values = np.zeros(self.test.shape[0])\n",
    "        cont = 0\n",
    "        for playlist in range(playlistsSize):\n",
    "            tracks = np.array(self.test[self.test['playlist_id']==playlist]['track_id'])\n",
    "            length = len(tracks)\n",
    "            if length > 0:\n",
    "                URM_test_row[cont:cont+length] = [playlist]*length\n",
    "                URM_test_col[cont:cont+length] = tracks\n",
    "                URM_test_values[cont:cont+length] = [1]*length\n",
    "                cont = cont + length\n",
    "        self.URM_test = sps.csr_matrix( (URM_test_values,(URM_test_row, URM_test_col)), shape=(playlistsSize, tracksSize))\n",
    "        return self.URM_test\n",
    "\n",
    "    def get_URM_train(self):\n",
    "        playlistsSize = len(self.get_playlists())\n",
    "        tracksSize = len(self.get_tracks())\n",
    "        URM_train_row = np.zeros(self.train.shape[0])\n",
    "        URM_train_col = np.zeros(self.train.shape[0])\n",
    "        URM_train_values = np.zeros(self.train.shape[0])\n",
    "        cont = 0\n",
    "        for playlist in range(playlistsSize):\n",
    "            tracks = np.array(self.train[self.train['playlist_id']==playlist]['track_id'])\n",
    "            length = len(tracks)\n",
    "            if length > 0:\n",
    "                URM_train_row[cont:cont+length] = [playlist]*length\n",
    "                URM_train_col[cont:cont+length] = tracks\n",
    "                URM_train_values[cont:cont+length] = [1]*length\n",
    "                cont = cont + length\n",
    "        self.URM_train = sps.csr_matrix( (URM_train_values,(URM_train_row, URM_train_col)), shape=(playlistsSize, tracksSize))\n",
    "        return self.URM_train\n",
    "    \n",
    "    def get_URM_transpose_train(self):\n",
    "        playlistsSize = len(self.get_playlists())\n",
    "        tracksSize = len(self.get_tracks())\n",
    "        URM_train_row = np.zeros(self.train.shape[0])\n",
    "        URM_train_col = np.zeros(self.train.shape[0])\n",
    "        URM_train_values = np.zeros(self.train.shape[0])\n",
    "        cont = 0\n",
    "        for track in range(tracksSize):\n",
    "            playlists = np.array(self.train[self.train['track_id']==track]['playlist_id'])\n",
    "            length = len(playlists)\n",
    "            if length > 0:\n",
    "                URM_train_row[cont:cont+length] = [track]*length\n",
    "                URM_train_col[cont:cont+length] = playlists\n",
    "                URM_train_values[cont:cont+length] = [1]*length\n",
    "                cont = cont + length\n",
    "        self.URM_train_transpose = sps.csr_matrix( (URM_train_values,(URM_train_row, URM_train_col)), shape=(tracksSize, playlistsSize))\n",
    "        return self.URM_train_transpose\n",
    "    \n",
    "    def get_ICM(self, a):\n",
    "        artists = self.tracks.reindex(columns=['track_id', 'artist_id'])\n",
    "        artists.sort_values(by='track_id', inplace=True)\n",
    "        artists_list = [[a] for a in artists['artist_id']]\n",
    "        icm_artists = MultiLabelBinarizer(classes=self.get_artists(), sparse_output=True).fit_transform(artists_list)\n",
    "        icm_artists_csr = icm_artists.tocsr()\n",
    "        #return icm_artists_csr\n",
    "        \n",
    "        albums = self.tracks.reindex(columns=['track_id', 'album_id'])\n",
    "        albums.sort_values(by='track_id', inplace=True)\n",
    "        albums_list = [[a] for a in albums['album_id']]\n",
    "        icm_albums = MultiLabelBinarizer(classes=self.get_albums(), sparse_output=True).fit_transform(albums_list)\n",
    "        icm_albums_csr = icm_albums.tocsr()\n",
    "        #return icm_albums_csr\n",
    "          \n",
    "        return sps.hstack((a*icm_artists_csr,icm_albums_csr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2b0c503d9f2f86dea9fbb443c1c3391c20fc925"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "class HybridRecommender(object):\n",
    "\n",
    "    def __init__(self, contentSimilarity, collaborativeSimilarity, userbasedsimilarity, a, b, c, builder):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.userbasedsimilarity = userbasedsimilarity\n",
    "        self.contentSimilarity = contentSimilarity\n",
    "        self.collaborativeSimilarity = collaborativeSimilarity\n",
    "        #self.bestSimilarTracks = a*contentSimilarity + b*collaborativeSimilarity\n",
    "        self.URM_transpose =  builder.URM_train_transpose\n",
    "        self.cont = -1\n",
    "\n",
    "\n",
    "    def calculate_rankings(self, matrix, tracks,weight):\n",
    "        tracksSet = set(tracks)\n",
    "        best = {}\n",
    "        temp = 1\n",
    "        minimum = 0\n",
    "        q = 1/(len(tracks)+1)**2\n",
    "        for track in tracks:\n",
    "            row_start = matrix.indptr[track]\n",
    "            row_end = matrix.indptr[track+1]\n",
    "            similarTracks = matrix.indices[row_start:row_end]\n",
    "            similarityValues = matrix.data[row_start:row_end]\n",
    "            for i in range(0, len(similarTracks)):\n",
    "                if not similarTracks[i] in tracksSet:\n",
    "                    if similarTracks[i] in best:\n",
    "                        best[similarTracks[i]]=best[similarTracks[i]]-similarityValues[i]*temp\n",
    "                    else:\n",
    "                        best[similarTracks[i]]=-1*similarityValues[i]*temp\n",
    "                    minimum = min(minimum, best[similarTracks[i]])\n",
    "            if self.cont < 5000:\n",
    "                temp -= q\n",
    "        for k in best:\n",
    "            best[k] = weight*best[k]/minimum*(-1)\n",
    "        return best\n",
    "\n",
    "\n",
    "    def userbased_calculate_ratings(self, playlist, tracks, matrix, weight, URM_transpose):\n",
    "        best = {}\n",
    "        minimum = 0\n",
    "        row_start = matrix.indptr[playlist]\n",
    "        row_end = matrix.indptr[playlist+1]\n",
    "        similarPlaylists = matrix.indices[row_start:row_end]\n",
    "        similarityValues = matrix.data[row_start:row_end]\n",
    "        values = {}\n",
    "        for i in range(len(similarPlaylists)):\n",
    "            values[similarPlaylists[i]] = similarityValues[i]\n",
    "        similarPlaylists = set(similarPlaylists)\n",
    "        for track in tracks:\n",
    "            row_start = URM_transpose.indptr[track]\n",
    "            row_end = URM_transpose.indptr[track+1]\n",
    "            playlistsForTrack = set(URM_transpose.indices[row_start:row_end])\n",
    "            playlists = similarPlaylists & playlistsForTrack\n",
    "            ctrl = False\n",
    "            for p in playlists:\n",
    "                if ctrl == True:\n",
    "                    best[track]=best[track]-values[p]\n",
    "                else:\n",
    "                    best[track]=-1*values[p]\n",
    "                    ctrl = True\n",
    "            if track in best:\n",
    "                minimum = min(minimum, best[track])\n",
    "        for k in best:\n",
    "            best[k] = weight*best[k]/minimum*(-1)\n",
    "        return best\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def recommend1(self, playlist, builder):\n",
    "        #print(playlist)\n",
    "        self.cont = self.cont + 1\n",
    "        tracks = builder.get_tracks_inside_playlist_train(playlist)\n",
    "        content_ratings = self.calculate_rankings(self.contentSimilarity, tracks, self.a)\n",
    "        collaborative_ratings = self.calculate_rankings(self.collaborativeSimilarity, tracks, self.b)\n",
    "        for k in content_ratings:\n",
    "            if k in collaborative_ratings:\n",
    "                collaborative_ratings[k] = collaborative_ratings[k] + content_ratings[k]\n",
    "            else:\n",
    "                collaborative_ratings[k] = content_ratings[k]\n",
    "        best = collaborative_ratings\n",
    "        userbased_ratings = self.userbased_calculate_ratings(playlist, best, self.userbasedsimilarity, self.c, self.URM_transpose)\n",
    "        for k in userbased_ratings:\n",
    "            if k in best:\n",
    "                best[k] = best[k] + userbased_ratings[k]\n",
    "        preSorted = [[v, k] for k,v in best.items()]\n",
    "        best = np.empty((max(11,len(preSorted)), 2), dtype=object)\n",
    "        for i in range(len(preSorted)):\n",
    "            best[i] = preSorted[i]\n",
    "        if len(preSorted) < 11:\n",
    "            for i in range(len(preSorted), 11):\n",
    "                best[i] = [0, random.randint(0, 20000)]\n",
    "            #print(best)\n",
    "        best = best[best[:,0].argpartition(10)][0:10]\n",
    "        #if playlist == 7:\n",
    "        #    print(best)\n",
    "        best = best[best[:,0].argsort()][:,1]\n",
    "        return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dce0bf9b045c0e4a6702c95f2122235e0d5ef84d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "\n",
    "\n",
    "def precision(is_relevant, relevant_items):\n",
    "\n",
    "    #is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "\n",
    "    return precision_score\n",
    "\n",
    "\n",
    "\n",
    "def recall(is_relevant, relevant_items):\n",
    "\n",
    "    #is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "\n",
    "    return recall_score\n",
    "\n",
    "\n",
    "\n",
    "def MAP(is_relevant, relevant_items):\n",
    "\n",
    "    #is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "\n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_algorithm(URM_test, recommender_object, builder):\n",
    "\n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_MAP = 0.0\n",
    "\n",
    "    num_eval = 0\n",
    "\n",
    "    URM_test = sps.csr_matrix(URM_test)\n",
    "\n",
    "    ordered_target_playlists = builder.get_ordered_target_playlists()\n",
    "    unordered_target_playlists = builder.get_unordered_target_playlists()\n",
    "\n",
    "    for i in range(len(ordered_target_playlists)):\n",
    "        \n",
    "        user_id = ordered_target_playlists[i]\n",
    "\n",
    "        start_pos = URM_test.indptr[user_id]\n",
    "        end_pos = URM_test.indptr[user_id+1]\n",
    "\n",
    "        if end_pos-start_pos>0:\n",
    "\n",
    "            relevant_items = URM_test.indices[start_pos:end_pos]\n",
    "\n",
    "            recommended_items = recommender_object.recommend1(user_id,builder)\n",
    "            num_eval+=1\n",
    "\n",
    "            is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "            cumulative_precision += precision(is_relevant, relevant_items)\n",
    "            cumulative_recall += recall(is_relevant, relevant_items)\n",
    "            cumulative_MAP += MAP(is_relevant, relevant_items)\n",
    "            if i == 0:\n",
    "                print(recommended_items)\n",
    "                print(relevant_items)\n",
    "            \n",
    "    #print(\"ordered finished\")\n",
    "        \n",
    "    for i in range(len(unordered_target_playlists)):\n",
    "        \n",
    "        user_id = unordered_target_playlists[i]\n",
    "\n",
    "        start_pos = URM_test.indptr[user_id]\n",
    "        end_pos = URM_test.indptr[user_id+1]\n",
    "\n",
    "        if end_pos-start_pos>0:\n",
    "\n",
    "            relevant_items = URM_test.indices[start_pos:end_pos]\n",
    "\n",
    "            recommended_items = recommender_object.recommend1(user_id,builder)\n",
    "            num_eval+=1\n",
    "\n",
    "            is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "\n",
    "            cumulative_precision += precision(is_relevant, relevant_items)\n",
    "            cumulative_recall += recall(is_relevant, relevant_items)\n",
    "            cumulative_MAP += MAP(is_relevant, relevant_items)\n",
    "\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    cumulative_MAP /= num_eval\n",
    "\n",
    "    print(\"Recommender performance is: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "        cumulative_precision, cumulative_recall, cumulative_MAP))\n",
    "\n",
    "    result_dict = {\n",
    "        \"precision\": cumulative_precision,\n",
    "        \"recall\": cumulative_recall,\n",
    "        \"MAP\": cumulative_MAP,\n",
    "    }\n",
    "\n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "local = True\n",
    "b = Builder(local)\n",
    "ICM = b.get_ICM(0.7)\n",
    "URM_train_transpose = b.get_URM_transpose_train()\n",
    "URM_train = b.get_URM_train()\n",
    "if local == True:\n",
    "    URM_test = b.get_URM_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d965336f16fe1043f43a76555a4fcf762923f648"
   },
   "outputs": [],
   "source": [
    "def compute_similarity():\n",
    "    recommender = ItemCBFKNNRecommender(URM_train, ICM)\n",
    "    contentSimilarity = recommender.fit(shrink=0.0, topK=200)\n",
    "    recommender = ItemCBFKNNRecommender(URM_train, URM_train_transpose)\n",
    "    collaborativeSimilarity = recommender.fit(shrink=5.0, topK=500)\n",
    "    recommender = ItemCBFKNNRecommender(URM_train, URM_train)\n",
    "    userbasedSimilarity = recommender.fit(shrink=5.0, topK=200)\n",
    "    return contentSimilarity, collaborativeSimilarity, userbasedSimilarity\n",
    "\n",
    "def run_local(a, a1, a2):\n",
    "    return evaluate_algorithm(URM_test, HybridRecommender(contentSimilarity, collaborativeSimilarity, userbasedSimilarity, a, a1, a2, b),b)#content value,collaborative value\n",
    "\n",
    "def run_online(a, a1, a2):\n",
    "    print_to_csv(contentSimilarity, collaborativeSimilarity, userbasedSimilarity, a, a1, a2)\n",
    "    \n",
    "def run(a, a1, a2):\n",
    "    if local == True:\n",
    "        run_local(a, a1, a2)\n",
    "    else:\n",
    "        run_online(a, a1, a2)\n",
    "\n",
    "def print_to_csv(contentSimilarity, collaborativeSimilarity, userbasedSimilarity, a, a1, a2):\n",
    "    file=open(\"hybrid-submission.csv\",'a')\n",
    "    file.write(\"playlist_id,track_ids\"+\"\\n\")\n",
    "    recommender = HybridRecommender(contentSimilarity, collaborativeSimilarity, userbasedSimilarity, a, a1, a2, b)\n",
    "    for playlist in b.get_ordered_target_playlists():\n",
    "        s = str(recommender.recommend1(playlist,b))\n",
    "        s = s[1:len(s)-1]\n",
    "        file.write(str(playlist)+\",\"+s+\"\\n\")\n",
    "    for playlist in b.get_unordered_target_playlists():\n",
    "        s = str(recommender.recommend1(playlist,b))\n",
    "        s = s[1:len(s)-1]\n",
    "        file.write(str(playlist)+\",\"+s+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1706eb96e0da4948cfd5c107af965c1d0831ec2c"
   },
   "outputs": [],
   "source": [
    " contentSimilarity, collaborativeSimilarity, userbasedSimilarity = compute_similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5292c95423f853bd29e3d496101417844f5db280"
   },
   "outputs": [],
   "source": [
    "for a in range(0, 5):\n",
    "    for a2 in range(0, 10):\n",
    "        print(str(a)+ \" \" + str(a2))\n",
    "        print(run_local(0.1+a*0.05, 1, 0.1+a2*0.07))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
